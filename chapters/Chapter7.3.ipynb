{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 7.3 - Spark Streaming\n",
    "\n",
    "Paul E. Anderson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ice Breaker\n",
    "\n",
    "Best breakfast burrito in town?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import os\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem Statement:\n",
    "* You are approached by a company who has a machine learning pipeline that is trained and tested on historical data. \n",
    "* This pipeline is used by the company to sort tweets into one of three categories which also have a corresponding numerical label in parentheses.\n",
    "    * Negative (0)\n",
    "    * Positive (1)\n",
    "    * Neutral (2)\n",
    "    \n",
    "The company has heard about your amazing skills as a Spark streaming expert. They would like you to take their pre-trained classifier and update it with new incoming data processed via Spark streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detours\n",
    "\n",
    "In order to implement our streaming approach, we need to take a couple of brief detours into machine learning. We need to answer the following questions:\n",
    "* How do we represent text as a vector of numbers such that a machine can mathematically learn from data?\n",
    "* How to use and evaluate an algorithm to predict numeric data into three categories (negative, positive, and neutral)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YbTiSkqNb75B"
   },
   "source": [
    "### Representing text as a vector using `scikit-learn`\n",
    "\n",
    "scikit-learn is a popular package for machine learning.\n",
    "\n",
    "We will use a class called `CountVectorizer` in `scikit-learn` to obtain what is called the term-frequency matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple famous book openings:\n",
    "\n",
    "> The story so far: in the beginning, the universe was created. This has made a lot of people very angry and been widely regarded as a bad move - The Restaurant at the End of the Universe by Douglas Adams (1980)\n",
    "\n",
    "> Whether I shall turn out to be the hero of my own life, or whether that station will be held by anybody else, these pages must show. — Charles Dickens, David Copperfield (1850)\n",
    "\n",
    "How will a computer understand these sentences when computers can only add/mult/compare numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fhl2Kwb5b75C"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x46 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 48 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "famous_book_openings = [\n",
    "    \"The story so far: in the beginning, the universe was created. This has made a lot of people very angry and been widely regarded as a bad move\",\n",
    "    \"Whether I shall turn out to be the hero of my own life, or whether that station will be held by anybody else, these pages must show.\"\n",
    "]\n",
    "\n",
    "vec = CountVectorizer()\n",
    "vec.fit(famous_book_openings) # This determines the vocabulary.\n",
    "tf_sparse = vec.transform(famous_book_openings)\n",
    "tf_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing in a readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>angry</th>\n",
       "      <th>anybody</th>\n",
       "      <th>as</th>\n",
       "      <th>bad</th>\n",
       "      <th>be</th>\n",
       "      <th>been</th>\n",
       "      <th>beginning</th>\n",
       "      <th>by</th>\n",
       "      <th>created</th>\n",
       "      <th>...</th>\n",
       "      <th>these</th>\n",
       "      <th>this</th>\n",
       "      <th>to</th>\n",
       "      <th>turn</th>\n",
       "      <th>universe</th>\n",
       "      <th>very</th>\n",
       "      <th>was</th>\n",
       "      <th>whether</th>\n",
       "      <th>widely</th>\n",
       "      <th>will</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  angry  anybody  as  bad  be  been  beginning  by  created  ...  these  \\\n",
       "0    1      1        0   1    1   0     1          1   0        1  ...      0   \n",
       "1    0      0        1   0    0   2     0          0   1        0  ...      1   \n",
       "\n",
       "   this  to  turn  universe  very  was  whether  widely  will  \n",
       "0     1   0     0         1     1    1        0       1     0  \n",
       "1     0   1     1         0     0    0        2       0     1  \n",
       "\n",
       "[2 rows x 46 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(\n",
    "    tf_sparse.todense(),\n",
    "    columns=vec.get_feature_names()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying this process to our twitter data\n",
    "We will do the following:\n",
    "1. Load the tweets into a dataframe\n",
    "2. Convert those tweets into a term-frequency matrix using the code from above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading tweets into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "header must be integer or list of integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f189e5c4d319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mhistorical_training_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mread_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{home}/csc-369-student/data/twitter_sentiment_analysis/historical/xa*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-f189e5c4d319>\u001b[0m in \u001b[0;36mread_files\u001b[0;34m(glob_pattern, columns)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhistorical_training_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ItemID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhistorical_training_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mhistorical_training_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1855\u001b[0m         \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1857\u001b[0;31m         \u001b[0mParserBase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1859\u001b[0m         \u001b[0;31m# #2442\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/tljh/user/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, kwds)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_integer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header must be integer or list of integers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m                 raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: header must be integer or list of integers"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "def read_files(glob_pattern,columns=['ItemID','Sentiment','SentimentText']):\n",
    "    files = glob.glob(glob_pattern)\n",
    "    files = sorted(files)\n",
    "    historical_training_data = None\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file,header=None)\n",
    "        columns).set_index('ItemID')\n",
    "        if historical_training_data is None:\n",
    "            historical_training_data = df\n",
    "        else:\n",
    "            historical_training_data = historical_training_data.append(df)\n",
    "    return historical_training_data\n",
    "\n",
    "read_files(f'{home}/csc-369-student/data/twitter_sentiment_analysis/historical/xa*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to a term frequency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer()\n",
    "vec.fit(historical_training_data['SentimentText']) # This determines the vocabulary.\n",
    "tf_sparse = vec.transform(historical_training_data['SentimentText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical model for prediction\n",
    "* We will use a multinomial Bayes classifier. \n",
    "* It is a statistical classifier that has good baseline performance for text analysis. \n",
    "* It's a classifier that you can update as new data arrives (i.e., online learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(tf_sparse,historical_training_data['Sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How well does this model predict on historical data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8847233977071632"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(model.predict(tf_sparse) == historical_training_data['Sentiment'])/len(historical_training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The usual SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Grab a streaming context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After a context is defined:\n",
    "* Define the input sources by creating input DStreams.\n",
    "* Define the streaming computations by applying transformation and output operations to DStreams.\n",
    "* Start receiving data and processing it using streamingContext.start().\n",
    "* Wait for the processing to be stopped (manually or due to any error) using streamingContext.awaitTermination().\n",
    "* The processing can be manually stopped using streamingContext.stop()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points to remember:\n",
    "* Once a context has been started, no new streaming computations can be set up or added to it.\n",
    "* Once a context has been stopped, it cannot be restarted.\n",
    "* Only one StreamingContext can be active in a JVM at the same time.\n",
    "* A SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped (without stopping the SparkContext) before the next StreamingContext is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "PORT=9999 # Change this to a unique port before running individually\n",
    "HOST=\"localhost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this command at the terminal and type in words and hit enter periodically:\n",
      "nc -lk 9999\n"
     ]
    }
   ],
   "source": [
    "print(\"Run this command at the terminal and type in words and hit enter periodically:\")\n",
    "print(f\"nc -lk {PORT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretized Streams (DStreams)\n",
    "* DStream is the basic abstraction provided by Spark Streaming\n",
    "* Continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream. \n",
    "* Internally, a DStream is represented by a continuous series of RDDs\n",
    "* Each RDD in a DStream contains data from a certain interval, as shown in the following figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Any operation applied on a DStream translates to operations on the underlying RDDs. \n",
    "* In our example of converting a stream of lines to words, the flatMap operation is applied on each RDD in the lines DStream to generate the RDDs of the words DStream. \n",
    "* This is shown in the following figure:\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream-ops.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:33:41\n",
      "-------------------------------------------\n",
      "('', 3)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:33:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:33:43\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:33:44\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:33:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:33:46\n",
      "-------------------------------------------\n",
      "('testing', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:33:47\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:33:48\n",
      "-------------------------------------------\n",
      "('test', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:33:49\n",
      "-------------------------------------------\n",
      "('testing', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:33:50\n",
      "-------------------------------------------\n",
      "('blah', 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = ssc.socketTextStream(HOST, PORT)\n",
    "counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "              .map(lambda word: (word, 1))\\\n",
    "              .reduceByKey(lambda a, b: a+b)\n",
    "counts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "import time; time.sleep(10)\n",
    "#ssc.awaitTerminationOrTimeout(60) # wait 60 seconds\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop and think:** What is missing in our previous example? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing is a lack of state. We process the lines in an RDD/DStream and print the results. What if we wanted to accumulate the word counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:41:50\n",
      "-------------------------------------------\n",
      "('hello', 1)\n",
      "('world', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:41:51\n",
      "-------------------------------------------\n",
      "('hello', 1)\n",
      "('world', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:41:52\n",
      "-------------------------------------------\n",
      "('hello', 1)\n",
      "('world', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:41:53\n",
      "-------------------------------------------\n",
      "('hello', 1)\n",
      "('world', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:41:54\n",
      "-------------------------------------------\n",
      "('hello', 1)\n",
      "('world', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:41:55\n",
      "-------------------------------------------\n",
      "('hello', 1)\n",
      "('world', 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "# RDD with initial state (key, value) pairs\n",
    "\n",
    "def updateFunc(new_values, last_sum):\n",
    "    return sum(new_values) + (last_sum or 0)\n",
    "\n",
    "lines = ssc.socketTextStream(HOST,PORT)\n",
    "running_counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                      .map(lambda word: (word, 1))\\\n",
    "                      .updateStateByKey(updateFunc, initialRDD=initialStateRDD)\n",
    "\n",
    "running_counts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "import time; time.sleep(5)\n",
    "#ssc.awaitTerminationOrTimeout(60) # wait 60 seconds\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring a directory\n",
    "\n",
    "You can monitor a directory and apply the same processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:11\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:13\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:14\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:15\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:16\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:17\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:18\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:19\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:20\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:21\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:22\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:23\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:24\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:25\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:26\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:27\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:28\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:29\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:30\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:31\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:32\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:33\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:34\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:35\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:36\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:37\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:38\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/tmp/add_books_here\"\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "# RDD with initial state (key, value) pairs\n",
    "\n",
    "def updateFunc(new_values, last_sum):\n",
    "    return sum(new_values) + (last_sum or 0)\n",
    "\n",
    "lines = ssc.textFileStream(data_dir)\n",
    "\n",
    "running_counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                      .map(lambda word: (word, 1))\\\n",
    "                      .updateStateByKey(updateFunc)\n",
    "\n",
    "running_counts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "import time; time.sleep(30)\n",
    "#ssc.awaitTerminationOrTimeout(60) # wait 60 seconds\n",
    "ssc.stop(stopSparkContext=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bridging Streaming and Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2021-11-03 10:59:49 =========\n",
      "========= 2021-11-03 10:59:50 =========\n",
      "========= 2021-11-03 10:59:51 =========\n",
      "========= 2021-11-03 10:59:52 =========\n",
      "========= 2021-11-03 10:59:53 =========\n",
      "========= 2021-11-03 10:59:54 =========\n",
      "========= 2021-11-03 10:59:55 =========\n",
      "========= 2021-11-03 10:59:56 =========\n",
      "========= 2021-11-03 10:59:57 =========\n",
      "========= 2021-11-03 10:59:58 =========\n",
      "========= 2021-11-03 10:59:59 =========\n",
      "========= 2021-11-03 11:00:00 =========\n",
      "========= 2021-11-03 11:00:01 =========\n",
      "+------------+-----+\n",
      "|        word|total|\n",
      "+------------+-----+\n",
      "|      online|   12|\n",
      "|          By|   95|\n",
      "|   emotions.|    5|\n",
      "|        some| 1472|\n",
      "|      Volume|    3|\n",
      "|       still|  432|\n",
      "|   connected|   26|\n",
      "|       turn,|   16|\n",
      "|        ‘and|  172|\n",
      "|         few|  225|\n",
      "|        hope|  208|\n",
      "|   solemnity|    6|\n",
      "|       those|  863|\n",
      "|     speak?’|    1|\n",
      "|    suppose;|    3|\n",
      "|    wanders,|    1|\n",
      "|      doubts|   20|\n",
      "|       inner|    3|\n",
      "|cross-barred|    1|\n",
      "|         fog|    4|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "========= 2021-11-03 11:00:02 =========\n",
      "========= 2021-11-03 11:00:03 =========\n",
      "========= 2021-11-03 11:00:04 =========\n",
      "========= 2021-11-03 11:00:05 =========\n",
      "========= 2021-11-03 11:00:06 =========\n",
      "========= 2021-11-03 11:00:07 =========\n",
      "========= 2021-11-03 11:00:08 =========\n",
      "========= 2021-11-03 11:00:09 =========\n",
      "========= 2021-11-03 11:00:10 =========\n",
      "========= 2021-11-03 11:00:11 =========\n",
      "========= 2021-11-03 11:00:12 =========\n",
      "========= 2021-11-03 11:00:13 =========\n",
      "========= 2021-11-03 11:00:14 =========\n",
      "========= 2021-11-03 11:00:15 =========\n",
      "========= 2021-11-03 11:00:16 =========\n",
      "========= 2021-11-03 11:00:17 =========\n",
      "========= 2021-11-03 11:00:18 =========\n",
      "========= 2021-11-03 11:00:19 =========\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/tmp/add_books_here\"\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import traceback\n",
    "\n",
    "# Lazily instantiated global instance of SparkSession\n",
    "def getSparkSessionInstance(sparkConf):\n",
    "    if (\"sparkSessionSingletonInstance\" not in globals()):\n",
    "        globals()[\"sparkSessionSingletonInstance\"] = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(conf=sparkConf) \\\n",
    "            .getOrCreate()\n",
    "    return globals()[\"sparkSessionSingletonInstance\"]\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "lines = ssc.textFileStream(data_dir)\n",
    "\n",
    "def process(time, rdd):\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    # Get the singleton instance of SparkSession\n",
    "    try:\n",
    "        spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "        # Convert RDD[String] to RDD[Row] to DataFrame\n",
    "        words = rdd.flatMap(lambda line: line.split(\" \")).map(lambda word: word)\n",
    "        rowRdd = words.map(lambda w: Row(word=w))\n",
    "        wordsDataFrame = spark.createDataFrame(rowRdd)\n",
    "\n",
    "        # Creates a temporary view using the DataFrame\n",
    "        wordsDataFrame.createOrReplaceTempView(\"words\")\n",
    "\n",
    "        # Do word count on table using SQL and print it\n",
    "        wordCountsDataFrame = spark.sql(\"select word, count(*) as total from words group by word\")\n",
    "        print(wordCountsDataFrame.show())\n",
    "    except Exception:\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "lines.foreachRDD(process)\n",
    "\n",
    "ssc.start()\n",
    "import time; time.sleep(30)\n",
    "#ssc.awaitTerminationOrTimeout(60) # wait 60 seconds\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,md,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
