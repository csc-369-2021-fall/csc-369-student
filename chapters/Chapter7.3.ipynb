{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 7.3 - Spark Streaming\n",
    "\n",
    "Paul E. Anderson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ice Breaker\n",
    "\n",
    "Best breakfast burrito in town?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "import os\n",
    "from pathlib import Path\n",
    "home = str(Path.home())\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem Statement:\n",
    "* You are approached by a company who has a machine learning pipeline that is trained and tested on historical data. \n",
    "* This pipeline is used by the company to sort tweets into one of three categories which also have a corresponding numerical label in parentheses.\n",
    "    * Negative (0)\n",
    "    * Positive (1)\n",
    "    * Neutral (2)\n",
    "    \n",
    "The company has heard about your amazing skills as a Spark streaming expert. They would like you to take their pre-trained classifier and update it with new incoming data processed via Spark streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detours\n",
    "\n",
    "In order to implement our streaming approach, we need to take a couple of brief detours into machine learning. We need to answer the following questions:\n",
    "* How do we represent text as a vector of numbers such that a machine can mathematically learn from data?\n",
    "* How to use and evaluate an algorithm to predict numeric data into three categories (negative, positive, and neutral)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YbTiSkqNb75B"
   },
   "source": [
    "### Representing text as a vector using `scikit-learn`\n",
    "\n",
    "scikit-learn is a popular package for machine learning.\n",
    "\n",
    "We will use a class called `CountVectorizer` in `scikit-learn` to obtain what is called the term-frequency matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple famous book openings:\n",
    "\n",
    "> The story so far: in the beginning, the universe was created. This has made a lot of people very angry and been widely regarded as a bad move - The Restaurant at the End of the Universe by Douglas Adams (1980)\n",
    "\n",
    "> Whether I shall turn out to be the hero of my own life, or whether that station will be held by anybody else, these pages must show. — Charles Dickens, David Copperfield (1850)\n",
    "\n",
    "How will a computer understand these sentences when computers can only add/mult/compare numbers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fhl2Kwb5b75C"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x46 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 48 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "famous_book_openings = [\n",
    "    \"The story so far: in the beginning, the universe was created. This has made a lot of people very angry and been widely regarded as a bad move\",\n",
    "    \"Whether I shall turn out to be the hero of my own life, or whether that station will be held by anybody else, these pages must show.\"\n",
    "]\n",
    "\n",
    "vec = CountVectorizer()\n",
    "vec.fit(famous_book_openings) # This determines the vocabulary.\n",
    "tf_sparse = vec.transform(famous_book_openings)\n",
    "tf_sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing in a readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>angry</th>\n",
       "      <th>anybody</th>\n",
       "      <th>as</th>\n",
       "      <th>bad</th>\n",
       "      <th>be</th>\n",
       "      <th>been</th>\n",
       "      <th>beginning</th>\n",
       "      <th>by</th>\n",
       "      <th>created</th>\n",
       "      <th>...</th>\n",
       "      <th>these</th>\n",
       "      <th>this</th>\n",
       "      <th>to</th>\n",
       "      <th>turn</th>\n",
       "      <th>universe</th>\n",
       "      <th>very</th>\n",
       "      <th>was</th>\n",
       "      <th>whether</th>\n",
       "      <th>widely</th>\n",
       "      <th>will</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  angry  anybody  as  bad  be  been  beginning  by  created  ...  these  \\\n",
       "0    1      1        0   1    1   0     1          1   0        1  ...      0   \n",
       "1    0      0        1   0    0   2     0          0   1        0  ...      1   \n",
       "\n",
       "   this  to  turn  universe  very  was  whether  widely  will  \n",
       "0     1   0     0         1     1    1        0       1     0  \n",
       "1     0   1     1         0     0    0        2       0     1  \n",
       "\n",
       "[2 rows x 46 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(\n",
    "    tf_sparse.todense(),\n",
    "    columns=vec.get_feature_names()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying this process to our twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ItemID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentiment                                      SentimentText\n",
       "ItemID                                                              \n",
       "1               0                       is so sad for my APL frie...\n",
       "2               0                     I missed the New Moon trail...\n",
       "3               1                            omg its already 7:30 :O\n",
       "4               0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "5               0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "files = glob.glob(f'{home}/csc-369-student/data/twitter_sentiment_analysis/xa*') + \\\n",
    "        glob.glob(f'{home}/csc-369-student/data/twitter_sentiment_analysis/xb*')\n",
    "files = sorted(files)\n",
    "historical_training_data = None\n",
    "for file in files:\n",
    "    if historical_training_data is None:\n",
    "        df = pd.read_csv(file).set_index('ItemID')\n",
    "        historical_training_data = df\n",
    "    else:\n",
    "        df = pd.read_csv(file,header=None)\n",
    "        df.columns = historical_training_data.reset_index().columns\n",
    "        historical_training_data = historical_training_data.append(df.set_index('ItemID'))\n",
    "historical_training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The usual SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Grab a streaming context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After a context is defined:\n",
    "* Define the input sources by creating input DStreams.\n",
    "* Define the streaming computations by applying transformation and output operations to DStreams.\n",
    "* Start receiving data and processing it using streamingContext.start().\n",
    "* Wait for the processing to be stopped (manually or due to any error) using streamingContext.awaitTermination().\n",
    "* The processing can be manually stopped using streamingContext.stop()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points to remember:\n",
    "* Once a context has been started, no new streaming computations can be set up or added to it.\n",
    "* Once a context has been stopped, it cannot be restarted.\n",
    "* Only one StreamingContext can be active in a JVM at the same time.\n",
    "* A SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped (without stopping the SparkContext) before the next StreamingContext is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "PORT=9999 # Change this to a unique port before running individually\n",
    "HOST=\"localhost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this command at the terminal and type in words and hit enter periodically:\n",
      "nc -lk 9999\n"
     ]
    }
   ],
   "source": [
    "print(\"Run this command at the terminal and type in words and hit enter periodically:\")\n",
    "print(f\"nc -lk {PORT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretized Streams (DStreams)\n",
    "* DStream is the basic abstraction provided by Spark Streaming\n",
    "* Continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream. \n",
    "* Internally, a DStream is represented by a continuous series of RDDs\n",
    "* Each RDD in a DStream contains data from a certain interval, as shown in the following figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Any operation applied on a DStream translates to operations on the underlying RDDs. \n",
    "* In our example of converting a stream of lines to words, the flatMap operation is applied on each RDD in the lines DStream to generate the RDDs of the words DStream. \n",
    "* This is shown in the following figure:\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream-ops.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:33:41\n",
      "-------------------------------------------\n",
      "('', 3)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:33:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:33:43\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:33:44\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:33:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:33:46\n",
      "-------------------------------------------\n",
      "('testing', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:33:47\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:33:48\n",
      "-------------------------------------------\n",
      "('test', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:33:49\n",
      "-------------------------------------------\n",
      "('testing', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:33:50\n",
      "-------------------------------------------\n",
      "('blah', 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = ssc.socketTextStream(HOST, PORT)\n",
    "counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "              .map(lambda word: (word, 1))\\\n",
    "              .reduceByKey(lambda a, b: a+b)\n",
    "counts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "import time; time.sleep(10)\n",
    "#ssc.awaitTerminationOrTimeout(60) # wait 60 seconds\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop and think:** What is missing in our previous example? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing is a lack of state. We process the lines in an RDD/DStream and print the results. What if we wanted to accumulate the word counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:41:50\n",
      "-------------------------------------------\n",
      "('hello', 1)\n",
      "('world', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:41:51\n",
      "-------------------------------------------\n",
      "('hello', 1)\n",
      "('world', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:41:52\n",
      "-------------------------------------------\n",
      "('hello', 1)\n",
      "('world', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:41:53\n",
      "-------------------------------------------\n",
      "('hello', 1)\n",
      "('world', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:41:54\n",
      "-------------------------------------------\n",
      "('hello', 1)\n",
      "('world', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:41:55\n",
      "-------------------------------------------\n",
      "('hello', 1)\n",
      "('world', 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "# RDD with initial state (key, value) pairs\n",
    "\n",
    "def updateFunc(new_values, last_sum):\n",
    "    return sum(new_values) + (last_sum or 0)\n",
    "\n",
    "lines = ssc.socketTextStream(HOST,PORT)\n",
    "running_counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                      .map(lambda word: (word, 1))\\\n",
    "                      .updateStateByKey(updateFunc, initialRDD=initialStateRDD)\n",
    "\n",
    "running_counts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "import time; time.sleep(5)\n",
    "#ssc.awaitTerminationOrTimeout(60) # wait 60 seconds\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring a directory\n",
    "\n",
    "You can monitor a directory and apply the same processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:09\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:10\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:11\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:12\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:13\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:14\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:15\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:16\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:17\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:18\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:19\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:20\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:21\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:22\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:23\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:24\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:25\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:26\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:27\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:28\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:29\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:30\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:31\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:32\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:33\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:34\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:35\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:36\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:37\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 10:46:38\n",
      "-------------------------------------------\n",
      "('', 3113)\n",
      "('EBook', 1)\n",
      "('of', 1343)\n",
      "('Affair', 5)\n",
      "('other', 58)\n",
      "('are', 183)\n",
      "('check', 3)\n",
      "('where', 47)\n",
      "('using', 5)\n",
      "('Styles', 26)\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/tmp/add_books_here\"\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "# RDD with initial state (key, value) pairs\n",
    "\n",
    "def updateFunc(new_values, last_sum):\n",
    "    return sum(new_values) + (last_sum or 0)\n",
    "\n",
    "lines = ssc.textFileStream(data_dir)\n",
    "\n",
    "running_counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                      .map(lambda word: (word, 1))\\\n",
    "                      .updateStateByKey(updateFunc)\n",
    "\n",
    "running_counts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "import time; time.sleep(30)\n",
    "#ssc.awaitTerminationOrTimeout(60) # wait 60 seconds\n",
    "ssc.stop(stopSparkContext=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bridging Streaming and Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2021-11-03 10:59:49 =========\n",
      "========= 2021-11-03 10:59:50 =========\n",
      "========= 2021-11-03 10:59:51 =========\n",
      "========= 2021-11-03 10:59:52 =========\n",
      "========= 2021-11-03 10:59:53 =========\n",
      "========= 2021-11-03 10:59:54 =========\n",
      "========= 2021-11-03 10:59:55 =========\n",
      "========= 2021-11-03 10:59:56 =========\n",
      "========= 2021-11-03 10:59:57 =========\n",
      "========= 2021-11-03 10:59:58 =========\n",
      "========= 2021-11-03 10:59:59 =========\n",
      "========= 2021-11-03 11:00:00 =========\n",
      "========= 2021-11-03 11:00:01 =========\n",
      "+------------+-----+\n",
      "|        word|total|\n",
      "+------------+-----+\n",
      "|      online|   12|\n",
      "|          By|   95|\n",
      "|   emotions.|    5|\n",
      "|        some| 1472|\n",
      "|      Volume|    3|\n",
      "|       still|  432|\n",
      "|   connected|   26|\n",
      "|       turn,|   16|\n",
      "|        ‘and|  172|\n",
      "|         few|  225|\n",
      "|        hope|  208|\n",
      "|   solemnity|    6|\n",
      "|       those|  863|\n",
      "|     speak?’|    1|\n",
      "|    suppose;|    3|\n",
      "|    wanders,|    1|\n",
      "|      doubts|   20|\n",
      "|       inner|    3|\n",
      "|cross-barred|    1|\n",
      "|         fog|    4|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "========= 2021-11-03 11:00:02 =========\n",
      "========= 2021-11-03 11:00:03 =========\n",
      "========= 2021-11-03 11:00:04 =========\n",
      "========= 2021-11-03 11:00:05 =========\n",
      "========= 2021-11-03 11:00:06 =========\n",
      "========= 2021-11-03 11:00:07 =========\n",
      "========= 2021-11-03 11:00:08 =========\n",
      "========= 2021-11-03 11:00:09 =========\n",
      "========= 2021-11-03 11:00:10 =========\n",
      "========= 2021-11-03 11:00:11 =========\n",
      "========= 2021-11-03 11:00:12 =========\n",
      "========= 2021-11-03 11:00:13 =========\n",
      "========= 2021-11-03 11:00:14 =========\n",
      "========= 2021-11-03 11:00:15 =========\n",
      "========= 2021-11-03 11:00:16 =========\n",
      "========= 2021-11-03 11:00:17 =========\n",
      "========= 2021-11-03 11:00:18 =========\n",
      "========= 2021-11-03 11:00:19 =========\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/tmp/add_books_here\"\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import traceback\n",
    "\n",
    "# Lazily instantiated global instance of SparkSession\n",
    "def getSparkSessionInstance(sparkConf):\n",
    "    if (\"sparkSessionSingletonInstance\" not in globals()):\n",
    "        globals()[\"sparkSessionSingletonInstance\"] = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(conf=sparkConf) \\\n",
    "            .getOrCreate()\n",
    "    return globals()[\"sparkSessionSingletonInstance\"]\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "lines = ssc.textFileStream(data_dir)\n",
    "\n",
    "def process(time, rdd):\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    # Get the singleton instance of SparkSession\n",
    "    try:\n",
    "        spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "        # Convert RDD[String] to RDD[Row] to DataFrame\n",
    "        words = rdd.flatMap(lambda line: line.split(\" \")).map(lambda word: word)\n",
    "        rowRdd = words.map(lambda w: Row(word=w))\n",
    "        wordsDataFrame = spark.createDataFrame(rowRdd)\n",
    "\n",
    "        # Creates a temporary view using the DataFrame\n",
    "        wordsDataFrame.createOrReplaceTempView(\"words\")\n",
    "\n",
    "        # Do word count on table using SQL and print it\n",
    "        wordCountsDataFrame = spark.sql(\"select word, count(*) as total from words group by word\")\n",
    "        print(wordCountsDataFrame.show())\n",
    "    except Exception:\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "lines.foreachRDD(process)\n",
    "\n",
    "ssc.start()\n",
    "import time; time.sleep(30)\n",
    "#ssc.awaitTerminationOrTimeout(60) # wait 60 seconds\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,md,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
