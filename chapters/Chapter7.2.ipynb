{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 7.2 - Spark Streaming\n",
    "\n",
    "Paul E. Anderson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ice Breaker\n",
    "\n",
    "What are you most looking forward to this holiday break?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Annotated Example: WordCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The usual SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "initialStateRDD = sc.parallelize([(u'hello', 1), (u'world', 1)]) # We'll use this later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Grab a streaming context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After a context is defined:\n",
    "* Define the input sources by creating input DStreams.\n",
    "* Define the streaming computations by applying transformation and output operations to DStreams.\n",
    "* Start receiving data and processing it using streamingContext.start().\n",
    "* Wait for the processing to be stopped (manually or due to any error) using streamingContext.awaitTermination().\n",
    "* The processing can be manually stopped using streamingContext.stop()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points to remember:\n",
    "* Once a context has been started, no new streaming computations can be set up or added to it.\n",
    "* Once a context has been stopped, it cannot be restarted.\n",
    "* Only one StreamingContext can be active in a JVM at the same time.\n",
    "* A SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext is stopped (without stopping the SparkContext) before the next StreamingContext is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "PORT=9999 # Change this to a unique port before running individually\n",
    "HOST=\"localhost\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this command at the terminal and type in words and hit enter periodically:\n",
      "nc -lk 9999\n"
     ]
    }
   ],
   "source": [
    "print(\"Run this command at the terminal and type in words and hit enter periodically:\")\n",
    "print(f\"nc -lk {PORT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretized Streams (DStreams)\n",
    "* DStream is the basic abstraction provided by Spark Streaming\n",
    "* Continuous stream of data, either the input data stream received from source, or the processed data stream generated by transforming the input stream. \n",
    "* Internally, a DStream is represented by a continuous series of RDDs\n",
    "* Each RDD in a DStream contains data from a certain interval, as shown in the following figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Any operation applied on a DStream translates to operations on the underlying RDDs. \n",
    "* In our example of converting a stream of lines to words, the flatMap operation is applied on each RDD in the lines DStream to generate the RDDs of the words DStream. \n",
    "* This is shown in the following figure:\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream-ops.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-11-03 09:30:19\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 09:30:20\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = ssc.socketTextStream(HOST, PORT)\n",
    "counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "              .map(lambda word: (word, 1))\\\n",
    "              .reduceByKey(lambda a, b: a+b)\n",
    "counts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "import time; time.sleep(1)\n",
    "#ssc.awaitTerminationOrTimeout(60) # wait 60 seconds\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stop and think:** What is missing in our previous example? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing is a lack of state. We process the lines in an RDD/DStream and print the results. What if we wanted to accumulate the word counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://129.65.17.223:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-11-03 09:32:21\n",
      "-------------------------------------------\n",
      "('', 3)\n",
      "('hello', 1)\n",
      "('world', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 09:32:22\n",
      "-------------------------------------------\n",
      "('', 3)\n",
      "('hello', 1)\n",
      "('world', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 09:32:23\n",
      "-------------------------------------------\n",
      "('', 3)\n",
      "('hello', 1)\n",
      "('world', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 09:32:24\n",
      "-------------------------------------------\n",
      "('', 3)\n",
      "('hello', 1)\n",
      "('world', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 09:32:25\n",
      "-------------------------------------------\n",
      "('', 3)\n",
      "('hello', 2)\n",
      "('world', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 09:32:26\n",
      "-------------------------------------------\n",
      "('', 3)\n",
      "('hello', 2)\n",
      "('world', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 09:32:27\n",
      "-------------------------------------------\n",
      "('', 3)\n",
      "('hello', 3)\n",
      "('world', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 09:32:28\n",
      "-------------------------------------------\n",
      "('', 3)\n",
      "('hello', 3)\n",
      "('world', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 09:32:29\n",
      "-------------------------------------------\n",
      "('', 3)\n",
      "('hello', 3)\n",
      "('world', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 09:32:30\n",
      "-------------------------------------------\n",
      "('', 3)\n",
      "('hello', 3)\n",
      "('world', 1)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-11-03 09:32:31\n",
      "-------------------------------------------\n",
      "('', 3)\n",
      "('hello', 3)\n",
      "('world', 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "# RDD with initial state (key, value) pairs\n",
    "\n",
    "def updateFunc(new_values, last_sum):\n",
    "    return sum(new_values) + (last_sum or 0)\n",
    "\n",
    "lines = ssc.socketTextStream(HOST,PORT)\n",
    "running_counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                      .map(lambda word: (word, 1))\\\n",
    "                      .updateStateByKey(updateFunc, initialRDD=initialStateRDD)\n",
    "\n",
    "running_counts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "import time; time.sleep(10)\n",
    "#ssc.awaitTerminationOrTimeout(60) # wait 60 seconds\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring a directory\n",
    "\n",
    "You can monitor a directory and apply the same processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-11-03 09:55:00\n",
      "-------------------------------------------\n",
      "('hello', 1)\n",
      "('world', 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/tmp/add_books_here\"\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "# RDD with initial state (key, value) pairs\n",
    "\n",
    "def updateFunc(new_values, last_sum):\n",
    "    return sum(new_values) + (last_sum or 0)\n",
    "\n",
    "lines = ssc.textFileStream(data_dir)\n",
    "\n",
    "running_counts = lines.flatMap(lambda line: line.split(\" \"))\\\n",
    "                      .map(lambda word: (word, 1))\\\n",
    "                      .updateStateByKey(updateFunc, initialRDD=initialStateRDD)\n",
    "\n",
    "running_counts.pprint()\n",
    "\n",
    "ssc.start()\n",
    "import time; time.sleep(1)\n",
    "#ssc.awaitTerminationOrTimeout(60) # wait 60 seconds\n",
    "ssc.stop(stopSparkContext=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bridging Streaming and Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2021-11-03 09:56:00 =========\n",
      "========= 2021-11-03 09:56:01 =========\n",
      "========= 2021-11-03 09:56:02 =========\n",
      "========= 2021-11-03 09:56:03 =========\n",
      "========= 2021-11-03 09:56:04 =========\n",
      "+--------------------+-----+\n",
      "|                word|total|\n",
      "+--------------------+-----+\n",
      "|              online|    4|\n",
      "|              Flower|    1|\n",
      "|                 ...|    4|\n",
      "|Descendants.--Exa...|    1|\n",
      "|               those|   82|\n",
      "|           destitute|   18|\n",
      "|         sand-hills;|    3|\n",
      "|        vicissitudes|    1|\n",
      "|                some|  160|\n",
      "|           Asiatics,|    1|\n",
      "|                 few|   74|\n",
      "|             pitcher|    1|\n",
      "|              travel|   10|\n",
      "|              freaks|    1|\n",
      "|               still|   43|\n",
      "|             tresses|    1|\n",
      "|              waters|   16|\n",
      "|                 art|    6|\n",
      "|              ransom|    1|\n",
      "|         unequivocal|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "========= 2021-11-03 09:56:05 =========\n",
      "========= 2021-11-03 09:56:06 =========\n",
      "========= 2021-11-03 09:56:07 =========\n",
      "========= 2021-11-03 09:56:08 =========\n",
      "========= 2021-11-03 09:56:09 =========\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import traceback\n",
    "\n",
    "# Lazily instantiated global instance of SparkSession\n",
    "def getSparkSessionInstance(sparkConf):\n",
    "    if (\"sparkSessionSingletonInstance\" not in globals()):\n",
    "        globals()[\"sparkSessionSingletonInstance\"] = SparkSession \\\n",
    "            .builder \\\n",
    "            .config(conf=sparkConf) \\\n",
    "            .getOrCreate()\n",
    "    return globals()[\"sparkSessionSingletonInstance\"]\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "lines = ssc.textFileStream(data_dir)\n",
    "\n",
    "def process(time, rdd):\n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    # Get the singleton instance of SparkSession\n",
    "    try:\n",
    "        spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "        # Convert RDD[String] to RDD[Row] to DataFrame\n",
    "        words = rdd.flatMap(lambda line: line.split(\" \")).map(lambda word: word)\n",
    "        rowRdd = words.map(lambda w: Row(word=w))\n",
    "        wordsDataFrame = spark.createDataFrame(rowRdd)\n",
    "\n",
    "        # Creates a temporary view using the DataFrame\n",
    "        wordsDataFrame.createOrReplaceTempView(\"words\")\n",
    "\n",
    "        # Do word count on table using SQL and print it\n",
    "        wordCountsDataFrame = spark.sql(\"select word, count(*) as total from words group by word\")\n",
    "        print(wordCountsDataFrame.show())\n",
    "    except Exception:\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "lines.foreachRDD(process)\n",
    "\n",
    "ssc.start()\n",
    "import time; time.sleep(10)\n",
    "#ssc.awaitTerminationOrTimeout(60) # wait 60 seconds\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,md,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
